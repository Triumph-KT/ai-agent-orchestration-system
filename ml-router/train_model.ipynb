{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1caba674-f414-4c10-8198-3a8596b5d947",
   "metadata": {},
   "source": [
    "### Cell 1: Imports\n",
    "- This cell imports all the libraries we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b78f485-38e9-496b-839b-78d2eef561cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0f0119-6214-40f0-836c-edb8171242f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  agent_id        task_type  capability_match  agent_current_load  \\\n",
      "0  agent_2   image_analysis                 0                   2   \n",
      "1  agent_1  code_generation                 0                   3   \n",
      "2  agent_2    data_analysis                 0                   3   \n",
      "3  agent_3   image_analysis                 1                   2   \n",
      "4  agent_1  code_generation                 0                   2   \n",
      "\n",
      "   agent_success_rate  task_complexity  duration_ms  \n",
      "0              0.9873              1.8        15402  \n",
      "1              0.9414              1.5        13372  \n",
      "2              0.9027              1.2        12861  \n",
      "3              0.9012              1.8         4609  \n",
      "4              0.9003              1.5        12047  \n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   agent_id            5000 non-null   object \n",
      " 1   task_type           5000 non-null   object \n",
      " 2   capability_match    5000 non-null   int64  \n",
      " 3   agent_current_load  5000 non-null   int64  \n",
      " 4   agent_success_rate  5000 non-null   float64\n",
      " 5   task_complexity     5000 non-null   float64\n",
      " 6   duration_ms         5000 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 273.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('training_data.csv')\n",
    "\n",
    "# Display the first 5 rows and data types\n",
    "print(df.head())\n",
    "print(\"\\nData Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ca92d-21d1-49a1-85b2-57d0877cf5bd",
   "metadata": {},
   "source": [
    "### Cell 3: Feature Engineering and Data Preparation\n",
    "The model needs all input to be numeric. We will convert the text-based columns (agent_id, task_type) into a numeric format using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee6516e-616e-4626-a918-8280671ff78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering pipeline created.\n"
     ]
    }
   ],
   "source": [
    "# Define features (X) and target (y)\n",
    "features = [\n",
    "    'agent_id', \n",
    "    'task_type', \n",
    "    'capability_match', \n",
    "    'agent_current_load', \n",
    "    'agent_success_rate', \n",
    "    'task_complexity'\n",
    "]\n",
    "target = 'duration_ms'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Create a column transformer to handle one-hot encoding for categorical features\n",
    "categorical_features = ['agent_id', 'task_type']\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"Feature engineering pipeline created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f4712-9f64-4729-9657-2e0267553809",
   "metadata": {},
   "source": [
    "### Cell 4: Train-Test Split\n",
    "We split our data into a training set (to teach the model) and a testing set (to see how well it performs on new data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b51b30-03a6-42f0-ab51-19730578c4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into 4000 training samples and 1000 testing samples.\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Data split into {len(X_train)} training samples and {len(X_test)} testing samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943fa379-0f89-4f0e-b9aa-f64c9bed16ef",
   "metadata": {},
   "source": [
    "### Cell 5: Define and Train the Model\n",
    "Here we create our Random Forest model and train it on the data. This may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb2f64e-22b3-45eb-948b-1a7a941c312f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Create the full model pipeline, including the preprocessor and the regressor\n",
    "model_pipeline = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddf7cc-76fc-41df-9ae9-dfaf2cf02e09",
   "metadata": {},
   "source": [
    "### Cell 6: Evaluate Model Performance\n",
    "Let's see how accurate our model is. We'll predict durations on the test data and check the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70b1647-be9c-4b03-8af0-96b18165d400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation complete.\n",
      "Mean Absolute Error: 415.34 ms\n",
      "This means our model's predictions are, on average, off by about 415.34 milliseconds.\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the error\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"Model evaluation complete.\")\n",
    "print(f\"Mean Absolute Error: {mae:.2f} ms\")\n",
    "print(f\"This means our model's predictions are, on average, off by about {mae:.2f} milliseconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a4d3c0-6b44-4547-8903-518e6322b0a0",
   "metadata": {},
   "source": [
    "### Cell 7: Save the Model for Production Use\n",
    "We save our trained model pipeline to a file so our Flask API can use it to make live predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad91804a-0c05-482e-9500-8387e1ad9acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pipeline saved to 'router_model.joblib'\n",
      "Model columns saved to 'model_columns.json'\n"
     ]
    }
   ],
   "source": [
    "# Save the entire pipeline (preprocessor + model) to a file\n",
    "model_filename = 'router_model.joblib'\n",
    "joblib.dump(model_pipeline, model_filename)\n",
    "\n",
    "# We also need to save the order of columns X was trained on\n",
    "model_columns = list(X.columns)\n",
    "with open('model_columns.json', 'w') as f:\n",
    "    json.dump(model_columns, f)\n",
    "\n",
    "print(f\"Model pipeline saved to '{model_filename}'\")\n",
    "print(f\"Model columns saved to 'model_columns.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658896a4-ef57-4217-9a4b-52f6b250a215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
